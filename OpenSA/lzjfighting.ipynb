{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    -*- coding: utf-8 -*-\n",
    "    @Time   :2022/04/12 17:10\n",
    "    @Author : Pengyou FU\n",
    "    @blogs  : https://blog.csdn.net/Echo_Code?spm=1000.2115.3001.5343\n",
    "    @github : https://github.com/FuSiry/OpenSA\n",
    "    @WeChat : Fu_siry\n",
    "    @License：Apache-2.0 license\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from DataLoad.DataLoad import SetSplit, LoadNirtest\n",
    "from Preprocessing.Preprocessing import Preprocessing\n",
    "from WaveSelect.WaveSelcet import SpctrumFeatureSelcet\n",
    "# from Plot.SpectrumPlot import plotspc\n",
    "# from Plot.SpectrumPlot import ClusterPlot\n",
    "from Simcalculation.SimCa import Simcalculation\n",
    "from Clustering.Cluster import Cluster\n",
    "from Regression.Rgs import QuantitativeAnalysis\n",
    "from Classification.Cls import QualitativeAnalysis\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "import airpls\n",
    "import matplotlib.pyplot as plt#导入强大的绘图库\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#光谱聚类分析\n",
    "def SpectralClusterAnalysis(data, label, ProcessMethods, FslecetedMethods, ClusterMethods):\n",
    "    \"\"\"\n",
    "     :param data: shape (n_samples, n_features), 光谱数据\n",
    "     :param label: shape (n_samples, ), 光谱数据对应的标签(理化性质)\n",
    "     :param ProcessMethods: string, 预处理的方法, 具体可以看预处理模块\n",
    "     :param FslecetedMethods: string, 光谱波长筛选的方法, 提供UVE、SPA、Lars、Cars、Pca\n",
    "     :param ClusterMethods : string, 聚类的方法，提供Kmeans聚类、FCM聚类\n",
    "     :return: Clusterlabels: 返回的隶属矩阵\n",
    "\n",
    "     \"\"\"\n",
    "    ProcesedData = Preprocessing(ProcessMethods, data)\n",
    "    FeatrueData, _ = SpctrumFeatureSelcet(FslecetedMethods, ProcesedData, label)\n",
    "    Clusterlabels = Cluster(ClusterMethods, FeatrueData)\n",
    "    #ClusterPlot(data, Clusterlabels)\n",
    "    return Clusterlabels\n",
    "\n",
    "# 光谱定量分析\n",
    "def SpectralQuantitativeAnalysis(data, label, ProcessMethods, FslecetedMethods, SetSplitMethods, model):\n",
    "\n",
    "    \"\"\"\n",
    "    :param data: shape (n_samples, n_features), 光谱数据\n",
    "    :param label: shape (n_samples, ), 光谱数据对应的标签(理化性质)\n",
    "    :param ProcessMethods: string, 预处理的方法, 具体可以看预处理模块\n",
    "    :param FslecetedMethods: string, 光谱波长筛选的方法, 提供UVE、SPA、Lars、Cars、Pca\n",
    "    :param SetSplitMethods : string, 划分数据集的方法, 提供随机划分、KS划分、SPXY划分\n",
    "    :param model : string, 定量分析模型, 包括ANN、PLS、SVR、ELM、CNN、SAE等，后续会不断补充完整\n",
    "    :return: Rmse: float, Rmse回归误差评估指标\n",
    "             R2: float, 回归拟合,\n",
    "             Mae: float, Mae回归误差评估指标\n",
    "    \"\"\"\n",
    "    # 要看效果好不好，如果效果不好，就不去基线了\n",
    "    # for i in range(data.shape[0]):\n",
    "    #     data[i] = airpls.airPLS_deBase(data[i])\n",
    "    # print(\"-----------去基线成功----------------\")\n",
    "    ProcesedData = Preprocessing(ProcessMethods, data)\n",
    "    FeatrueData, labels = SpctrumFeatureSelcet(FslecetedMethods, ProcesedData, label)\n",
    "    # print(\"---------------------------------------\")\n",
    "    # print(FeatrueData.shape)\n",
    "    # print(labels.shape)\n",
    "    # print(\"---------------------------------------\")\n",
    "\n",
    "    # sleep(100)\n",
    "    # 将特征值从很多很多降到了固定的 x个               def Pca(X, nums=20):\n",
    "    X_train, X_test, y_train, y_test = SetSplit(SetSplitMethods, FeatrueData, labels, test_size=0.2, randomseed=123)\n",
    "    # reg = LazyRegressor(ignore_warnings=False, custom_metric=None)\n",
    "    # models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    # print(models)\n",
    "    # best_model = models.iloc[0]\n",
    "    Rmse, R2, Mae = QuantitativeAnalysis(model, X_train, X_test, y_train, y_test ) \n",
    "    # model.save('model_savedmodel', save_format='tf')  \n",
    "    return Rmse, R2, Mae\n",
    "\n",
    "# 光谱定性分析\n",
    "def SpectralQualitativeAnalysis(data, label, ProcessMethods, FslecetedMethods, SetSplitMethods, model):\n",
    "\n",
    "    \"\"\"\n",
    "    :param data: shape (n_samples, n_features), 光谱数据\n",
    "    :param label: shape (n_samples, ), 光谱数据对应的标签(理化性质)\n",
    "    :param ProcessMethods: string, 预处理的方法, 具体可以看预处理模块\n",
    "    :param FslecetedMethods: string, 光谱波长筛选的方法, 提供UVE、SPA、Lars、Cars、Pca\n",
    "    :param SetSplitMethods : string, 划分数据集的方法, 提供随机划分、KS划分、SPXY划分\n",
    "    :param model : string, 定性分析模型, 包括ANN、PLS_DA、SVM、RF、CNN、SAE等，后续会不断补充完整\n",
    "    :return: acc： float, 分类准确率\n",
    "    \"\"\"\n",
    "\n",
    "    ProcesedData = Preprocessing(ProcessMethods, data)\n",
    "    FeatrueData, labels = SpctrumFeatureSelcet(FslecetedMethods, ProcesedData, label)\n",
    "    X_train, X_test, y_train, y_test = SetSplit(SetSplitMethods, FeatrueData, labels, test_size=0.2, randomseed=123)\n",
    "    acc = QualitativeAnalysis(model, X_train, X_test, y_train, y_test )\n",
    "    # model.save('model_savedmodel', save_format='tf')  \n",
    "    \n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "(655, 649)\n",
      "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
      "yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n",
      "(655, 2)\n",
      "yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (524, 2) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 29\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# for  i,element in enumerate(data2):\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#     if(i==5): break\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# # RMSE, R2, MAE = SpectralQuantitativeAnalysis(data2, label2, \"SG\", \"None\", \"ks\", \"CNN\")\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# RMSE, R2, MAE = SpectralQuantitativeAnalysis(data2, label2, \"SG\", \"None\", \"ks\", \"CNN\")\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     RMSE, R2, MAE \u001b[38;5;241m=\u001b[39m SpectralQuantitativeAnalysis(data2, label2, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSG\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Pca RMSE:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m R2:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, MAE:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m of result!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(RMSE, R2, MAE))\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 50\u001b[0m, in \u001b[0;36mSpectralQuantitativeAnalysis\u001b[1;34m(data, label, ProcessMethods, FslecetedMethods, SetSplitMethods, model)\u001b[0m\n\u001b[0;32m     45\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m SetSplit(SetSplitMethods, FeatrueData, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, randomseed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# reg = LazyRegressor(ignore_warnings=False, custom_metric=None)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# models, predictions = reg.fit(X_train, X_test, y_train, y_test)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# print(models)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# best_model = models.iloc[0]\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m Rmse, R2, Mae \u001b[38;5;241m=\u001b[39m QuantitativeAnalysis(model, X_train, X_test, y_train, y_test ) \n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# model.save('model_savedmodel', save_format='tf')  \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Rmse, R2, Mae\n",
      "File \u001b[1;32mf:\\github\\graduate-code\\OpenSA\\OpenSA\\Regression\\Rgs.py:32\u001b[0m, in \u001b[0;36mQuantitativeAnalysis\u001b[1;34m(model, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     30\u001b[0m     Rmse, R2, Mae \u001b[38;5;241m=\u001b[39m Anngression(X_train, X_test, y_train, y_test)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m     Rmse, R2, Mae \u001b[38;5;241m=\u001b[39m Svregression(X_train, X_test, y_train, y_test)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mELM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     34\u001b[0m     Rmse, R2, Mae \u001b[38;5;241m=\u001b[39m ELM(X_train, X_test, y_train, y_test)\n",
      "File \u001b[1;32mf:\\github\\graduate-code\\OpenSA\\OpenSA\\Regression\\ClassicRgs.py:188\u001b[0m, in \u001b[0;36mSvregression\u001b[1;34m(X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mSvregression\u001b[39m(X_train, X_test, y_train, y_test):\n\u001b[0;32m    187\u001b[0m     model \u001b[38;5;241m=\u001b[39m SVR(C\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-07\u001b[39m, kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;66;03m# predict the values\u001b[39;00m\n\u001b[0;32m    191\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\svm\\_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    191\u001b[0m         X,\n\u001b[0;32m    192\u001b[0m         y,\n\u001b[0;32m    193\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    194\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    195\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m     )\n\u001b[0;32m    199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_targets(y)\n\u001b[0;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[0;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    203\u001b[0m )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1163\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[0;32m   1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[1;32m-> 1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n\u001b[0;32m   1167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184\u001b[0m, in \u001b[0;36m_check_y\u001b[1;34m(y, multi_output, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1183\u001b[0m     estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m-> 1184\u001b[0m     y \u001b[38;5;241m=\u001b[39m column_or_1d(y, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1185\u001b[0m     _assert_all_finite(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, estimator_name\u001b[38;5;241m=\u001b[39mestimator_name)\n\u001b[0;32m   1186\u001b[0m     _ensure_no_complex_data(y)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1245\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1234\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1235\u001b[0m             (\n\u001b[0;32m   1236\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1241\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   1242\u001b[0m         )\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m-> 1245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[0;32m   1247\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (524, 2) instead."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # ## 载入原始数据并可视化\n",
    "    # data1, label1 = LoadNirtest('Cls')\n",
    "    # #plotspc(data1, \"raw specturm\")\n",
    "    # # 光谱定性分析演示\n",
    "    # # 示意1: 预处理算法:MSC , 波长筛选算法: 不使用, 全波长建模, 数据集划分:随机划分, 定性分析模型: RF\n",
    "    # acc = SpectralQualitativeAnalysis(data1, label1, \"MSC\", \"Lars\", \"random\", \"PLS_DA\")\n",
    "    # print(\"The acc:{} of result!\".format(acc))\n",
    "    mode =\"dingliang\"\n",
    "    if(mode==\"dingliang\"):\n",
    "    ## 载入原始数据并可视化\n",
    "    # 光谱定量分析演示\n",
    "    # 示意1: 预处理算法:MSC , 波长筛选算法: Uve, 数据集划分:KS, 定性分量模型: SVR\n",
    "        data2, label2 = LoadNirtest('Rgs')\n",
    "        print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")  \n",
    "        print(data2.shape)\n",
    "        print(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "        print(\"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\")\n",
    "        print(label2.shape) \n",
    "        print(\"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\")\n",
    "        # for  i,element in enumerate(data2):\n",
    "        #     if(i==5): break\n",
    "        #     else:\n",
    "        #         plt.plot([i for i in range(len(element))],element)\n",
    "                 \n",
    "        # # RMSE, R2, MAE = SpectralQuantitativeAnalysis(data2, label2, \"SG\", \"None\", \"ks\", \"CNN\")\n",
    "        # RMSE, R2, MAE = SpectralQuantitativeAnalysis(data2, label2, \"SG\", \"None\", \"ks\", \"CNN\")\n",
    "        RMSE, R2, MAE = SpectralQuantitativeAnalysis(data2, label2, \"SG\", \"None\", \"ks\", \"Lazy\")\n",
    "\n",
    "        print(\"The Pca RMSE:{} R2:{}, MAE:{} of result!\".format(RMSE, R2, MAE))\n",
    "    else:\n",
    "        data1, label1 = LoadNirtest('Cls')\n",
    "        acc = SpectralQualitativeAnalysis(data1, label1, \"SNV\", \"None\", \"random\", \"ANN\")\n",
    "        print(\"The acc:{} of result!\".format(acc))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 将数据进行归一化\u001b[39;00m\n\u001b[0;32m      5\u001b[0m List_Data_Scaler\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m  i,element \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data2):\n\u001b[0;32m      7\u001b[0m     meantemp \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(data2[i]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      9\u001b[0m     meantemp \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(meantemp)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 创建MinMaxScaler对象\n",
    "scaler = MinMaxScaler()\n",
    "# 将数据进行归一化\n",
    "List_Data_Scaler=[]\n",
    "for  i,element in enumerate(data2):\n",
    "    meantemp = scaler.fit_transform(data2[i].reshape(-1,1))\n",
    "   \n",
    "    meantemp = scaler.inverse_transform(meantemp)\n",
    "    List_Data_Scaler.append(meantemp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  i,element in enumerate(List_Data_Scaler):\n",
    "    if(i==5): break\n",
    "    else:\n",
    "        plt.plot([i for i in range(len(element))],element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy_arr = np.array(List_Data_Scaler)\n",
    "Y_numpy_arr = np.array(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义LSTM模型类\n",
    "import torch#一个深度学习的库Pytorch\n",
    "import torch.nn as nn#neural network,神经网络\n",
    "import torch.optim as optim#一个实现了各种优化算法的库\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size) # 初始化隐藏状态h0\n",
    "        c0 = torch.zeros(self.num_layers,x.size(0), self.hidden_size)  # 初始化记忆状态c0\n",
    "        #print(f\"x.shape:{x.shape},h0.shape:{h0.shape},c0.shape:{c0.shape}\")\n",
    "        out, _ = self.lstm(x, (h0, c0))  # LSTM前向传播\n",
    "        out = self.fc(out[:, -1, :])  # 取最后一个时间步的输出作为预测结果\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置随机种子\n",
    "import random\n",
    "torch.backends.cudnn.deterministic = True#将cudnn框架中的随机数生成器设为确定性模式\n",
    "torch.backends.cudnn.benchmark = False#关闭CuDNN框架的自动寻找最优卷积算法的功能，以避免不同的算法对结果产生影响\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X =X_numpy_arr\n",
    "test_y =Y_numpy_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatrueData, labels = SpctrumFeatureSelcet(\"None\", test_X, test_y)\n",
    "X_train, X_test, y_train, y_test = SetSplit(\"random\", FeatrueData, labels, test_size=0.2, randomseed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_X1=torch.Tensor(X_test)\n",
    "test_y1=torch.Tensor(y_test)\n",
    "train_X=torch.Tensor(X_train)\n",
    "train_y=torch.Tensor(y_train)\n",
    "# X_train, X_test, y_train, y_test = SetSplit(\"random\", FeatrueData, labels, test_size=0.2, randomseed=123)\n",
    "\n",
    "\n",
    "# 定义输入、隐藏状态和输出维度\n",
    "input_size = 1  # 输入特征维度\n",
    "hidden_size = 64  # LSTM隐藏状态维度\n",
    "num_layers = 5  # LSTM层数\n",
    "output_size = 1  # 输出维度（预测目标维度）\n",
    "\n",
    "# 创建LSTM模型实例\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
    "\n",
    "#训练周期为500次\n",
    "num_epochs=500\n",
    "batch_size=64#一次训练的数量\n",
    "#优化器\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.0001,betas=(0.5,0.999))\n",
    "#损失函数\n",
    "criterion=nn.MSELoss()\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "\n",
    "print(f\"start\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    random_num=[i for i in range(len(train_X))]\n",
    "    np.random.shuffle(random_num)\n",
    "    \n",
    "    train_X=train_X[random_num]\n",
    "    train_y=train_y[random_num]\n",
    "    \n",
    "    train_X1=torch.Tensor(train_X[:batch_size])\n",
    "    train_y1=torch.Tensor(train_y[:batch_size])\n",
    "    \n",
    "    #训练\n",
    "    model.train()\n",
    "    #将梯度清空\n",
    "    optimizer.zero_grad()\n",
    "    #将数据放进去训练\n",
    "    output=model(train_X1)\n",
    "    #计算每次的损失函数\n",
    "    train_loss=criterion(output,train_y1)\n",
    "    #反向传播\n",
    "    train_loss.backward()\n",
    "    #优化器进行优化(梯度下降,降低误差)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch%2==0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output=model(test_X1)\n",
    "            test_loss=criterion(output,test_y1)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"epoch:{epoch},train_loss:{train_loss},test_loss:{test_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
